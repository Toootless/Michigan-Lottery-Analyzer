#!/usr/bin/env python3
"""
Historical Data Investigation for LotteryUSA.com
Check how far back we can get lottery numbers and create a local database system
"""

import requests
from bs4 import BeautifulSoup
import re
from datetime import datetime, timedelta
import sqlite3
import json
from typing import Dict, Any, List, Optional, Tuple
import time

class LotteryHistoricalInvestigator:
    """Investigate historical data availability and create local database"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.9',
        })
        
        # Database setup
        self.db_path = "lottery_database.sqlite"
        self.init_database()
    
    def init_database(self):
        """Initialize SQLite database for lottery results"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS lottery_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                game TEXT NOT NULL,
                draw_date DATE NOT NULL,
                numbers TEXT NOT NULL,
                bonus_numbers TEXT,
                jackpot_amount REAL,
                source TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                UNIQUE(game, draw_date, numbers)
            )
        ''')
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS data_sources (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_name TEXT UNIQUE NOT NULL,
                base_url TEXT NOT NULL,
                last_updated TIMESTAMP,
                status TEXT DEFAULT 'active'
            )
        ''')
        
        # Insert known data sources
        cursor.execute('''
            INSERT OR IGNORE INTO data_sources (source_name, base_url) 
            VALUES ('lotteryusa', 'https://www.lotteryusa.com/michigan/')
        ''')
        
        conn.commit()
        conn.close()
        print(f"‚úÖ Database initialized: {self.db_path}")
    
    def investigate_historical_range(self, game: str = "daily-4") -> Dict[str, Any]:
        """Investigate how far back LotteryUSA.com goes for a specific game"""
        print(f"\nüîç Investigating historical range for {game}")
        print("=" * 50)
        
        base_url = f"https://www.lotteryusa.com/michigan/{game}/results"
        investigation_results = {
            'game': game,
            'earliest_date': None,
            'latest_date': None,
            'total_years': 0,
            'sample_dates': [],
            'pagination_info': {},
            'data_availability': []
        }
        
        # Check main results page
        print(f"üì° Checking main page: {base_url}")
        try:
            response = self.session.get(base_url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Look for pagination or year navigation
            pagination = self._find_pagination_info(soup)
            investigation_results['pagination_info'] = pagination
            
            # Extract recent dates and numbers
            recent_results = self._extract_recent_results(soup, game)
            investigation_results['sample_dates'] = recent_results
            
            if recent_results:
                investigation_results['latest_date'] = recent_results[0]['date']
                print(f"‚úÖ Latest result: {recent_results[0]}")
            
        except Exception as e:
            print(f"‚ùå Error accessing main page: {e}")
            return investigation_results
        
        # Try different URL patterns for historical data
        historical_patterns = [
            f"{base_url}/2021",
            f"{base_url}?year=2021", 
            f"{base_url}/archive/2021",
            f"https://www.lotteryusa.com/michigan/{game}/2021",
            f"https://www.lotteryusa.com/michigan/{game}/results/2021",
        ]
        
        for year in range(2021, 2025):  # Check 2021-2024
            year_available = False
            for pattern in historical_patterns:
                url = pattern.replace("2021", str(year))
                try:
                    print(f"   üì° Trying: {url}")
                    response = self.session.get(url, timeout=10)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        year_results = self._extract_recent_results(soup, game)
                        
                        if year_results:
                            print(f"   ‚úÖ Found {len(year_results)} results for {year}")
                            investigation_results['data_availability'].append({
                                'year': year,
                                'url': url,
                                'count': len(year_results),
                                'sample': year_results[:3]
                            })
                            year_available = True
                            
                            # Update earliest date
                            if year_results:
                                earliest_in_year = min(r['date'] for r in year_results if r['date'])
                                if not investigation_results['earliest_date'] or earliest_in_year < investigation_results['earliest_date']:
                                    investigation_results['earliest_date'] = earliest_in_year
                            break
                
                except Exception as e:
                    print(f"   ‚ùå {url}: {e}")
                    continue
            
            if not year_available:
                print(f"   ‚ö†Ô∏è  No data found for {year}")
            
            time.sleep(0.5)  # Be respectful to the server
        
        # Calculate total coverage
        if investigation_results['earliest_date'] and investigation_results['latest_date']:
            try:
                earliest = datetime.strptime(investigation_results['earliest_date'], "%Y-%m-%d")
                latest = datetime.strptime(investigation_results['latest_date'], "%Y-%m-%d")
                investigation_results['total_years'] = (latest - earliest).days / 365.25
            except:
                pass
        
        return investigation_results
    
    def _find_pagination_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """Look for pagination or navigation information"""
        pagination_info = {
            'has_pagination': False,
            'page_links': [],
            'year_links': [],
            'navigation_type': 'unknown'
        }
        
        # Look for pagination elements
        pagination_selectors = [
            '.pagination',
            '.page-navigation', 
            '[class*="pag"]',
            '.nav-links',
            '.page-links'
        ]
        
        for selector in pagination_selectors:
            elements = soup.select(selector)
            if elements:
                pagination_info['has_pagination'] = True
                pagination_info['navigation_type'] = 'pagination'
                
                for elem in elements:
                    links = elem.find_all('a', href=True)
                    for link in links:
                        href = link.get('href')
                        text = link.get_text(strip=True)
                        if href and text:
                            pagination_info['page_links'].append({'url': href, 'text': text})
                break
        
        # Look for year navigation
        year_pattern = r'(20\d{2})'
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href')
            text = link.get_text(strip=True)
            
            if re.search(year_pattern, href) or re.search(year_pattern, text):
                pagination_info['year_links'].append({'url': href, 'text': text})
        
        if pagination_info['year_links']:
            pagination_info['navigation_type'] = 'year_based'
        
        return pagination_info
    
    def _extract_recent_results(self, soup: BeautifulSoup, game: str) -> List[Dict[str, Any]]:
        """Extract lottery results from a page"""
        results = []
        
        # Strategy 1: Look for result tables
        tables = soup.find_all('table')
        for table in tables:
            rows = table.find_all('tr')
            for row in rows[1:]:  # Skip header
                cells = row.find_all(['td', 'th'])
                if len(cells) >= 2:
                    # Try to extract date and numbers
                    date_text = cells[0].get_text(strip=True)
                    numbers_text = ' '.join(cell.get_text(strip=True) for cell in cells[1:])
                    
                    # Extract date
                    date = self._parse_date(date_text)
                    
                    # Extract numbers
                    numbers = self._extract_numbers_from_text(numbers_text, game)
                    
                    if date and numbers:
                        results.append({
                            'date': date,
                            'numbers': numbers,
                            'raw_date': date_text,
                            'raw_numbers': numbers_text
                        })
        
        # Strategy 2: Look for result containers
        if not results:
            result_containers = soup.select('.result, .draw-result, [class*="result"]')
            for container in result_containers:
                text = container.get_text()
                
                # Try to extract date and numbers from container text
                date = self._parse_date(text)
                numbers = self._extract_numbers_from_text(text, game)
                
                if date and numbers:
                    results.append({
                        'date': date,
                        'numbers': numbers,
                        'raw_text': text[:100]
                    })
        
        return results[:20]  # Return up to 20 most recent results
    
    def _parse_date(self, date_text: str) -> Optional[str]:
        """Parse various date formats and return YYYY-MM-DD"""
        if not date_text:
            return None
        
        # Common date patterns
        patterns = [
            r'(\d{4})-(\d{2})-(\d{2})',  # 2021-12-31
            r'(\d{2})/(\d{2})/(\d{4})',  # 12/31/2021
            r'(\d{1,2})/(\d{1,2})/(\d{4})',  # 1/1/2021
            r'(\w+)\s+(\d{1,2}),?\s+(\d{4})',  # December 31, 2021
            r'(\d{1,2})\s+(\w+)\s+(\d{4})',  # 31 December 2021
        ]
        
        for pattern in patterns:
            match = re.search(pattern, date_text)
            if match:
                try:
                    groups = match.groups()
                    if len(groups) == 3:
                        if pattern == patterns[0]:  # Already YYYY-MM-DD
                            return f"{groups[0]}-{groups[1]}-{groups[2]}"
                        elif pattern in [patterns[1], patterns[2]]:  # MM/DD/YYYY
                            month, day, year = groups
                            return f"{year}-{month.zfill(2)}-{day.zfill(2)}"
                        # Add more date parsing logic as needed
                except:
                    continue
        
        return None
    
    def _extract_numbers_from_text(self, text: str, game: str) -> List[int]:
        """Extract lottery numbers from text"""
        if not text:
            return []
        
        # Extract all numbers from text
        all_numbers = re.findall(r'\b\d+\b', text)
        
        if not all_numbers:
            return []
        
        try:
            numbers = [int(n) for n in all_numbers]
            
            # Game-specific validation and filtering
            if game == "daily-4":
                # Look for 4 single digits
                valid_numbers = [n for n in numbers if 0 <= n <= 9]
                if len(valid_numbers) >= 4:
                    return valid_numbers[:4]
            elif game == "daily-3":
                valid_numbers = [n for n in numbers if 0 <= n <= 9]
                if len(valid_numbers) >= 3:
                    return valid_numbers[:3]
            elif game == "fantasy-5":
                valid_numbers = [n for n in numbers if 1 <= n <= 39]
                if len(valid_numbers) >= 5:
                    return sorted(valid_numbers[:5])
            
            return numbers[:10]  # Return first 10 numbers found
            
        except ValueError:
            return []
    
    def create_daily_update_system(self) -> str:
        """Create a daily update system script"""
        update_script = """#!/usr/bin/env python3
# Daily Lottery Results Updater
# Automatically fetch and store daily lottery results

import sqlite3
import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime, timedelta
import time
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('lottery_updater.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DailyLotteryUpdater:
    def __init__(self, db_path="lottery_database.sqlite"):
        self.db_path = db_path
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def update_all_games(self):
        games = ["daily-4", "daily-3", "fantasy-5", "lotto-47"]
        logger.info("Starting daily lottery update...")
        
        for game in games:
            try:
                self.update_game(game)
                time.sleep(2)
            except Exception as e:
                logger.error(f"Failed to update {game}: {e}")
        
        logger.info("Daily update completed")
    
    def update_game(self, game: str):
        url = f"https://www.lotteryusa.com/michigan/{game}/results"
        
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = self._extract_latest_results(soup, game)
            
            if results:
                self.store_results(results, game)
                logger.info(f"Updated {game}: {len(results)} new results")
            else:
                logger.warning(f"No new results found for {game}")
                
        except Exception as e:
            logger.error(f"Error updating {game}: {e}")
    
    def _extract_latest_results(self, soup, game):
        return []  # Simplified for this example
    
    def store_results(self, results: list, game: str):
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        for result in results:
            try:
                cursor.execute('''
                    INSERT OR IGNORE INTO lottery_results 
                    (game, draw_date, numbers, source) 
                    VALUES (?, ?, ?, ?)
                ''', (game, result['date'], json.dumps(result['numbers']), 'lotteryusa'))
            except Exception as e:
                logger.error(f"Error storing result: {e}")
        
        conn.commit()
        conn.close()

if __name__ == "__main__":
    updater = DailyLotteryUpdater()
    updater.update_all_games()
"""
        
        with open("daily_lottery_updater.py", "w", encoding="utf-8") as f:
            f.write(update_script)
        
        return "daily_lottery_updater.py"

def main():
    """Main investigation function"""
    print("üîç Michigan Lottery Historical Data Investigation")
    print("=" * 60)
    
    investigator = LotteryHistoricalInvestigator()
    
    # Investigate different games
    games_to_check = ["daily-4", "daily-3", "fantasy-5"]
    
    for game in games_to_check:
        print(f"\n{'='*60}")
        results = investigator.investigate_historical_range(game)
        
        print(f"\nüìä INVESTIGATION RESULTS for {game.upper()}:")
        print(f"Earliest Date Found: {results['earliest_date']}")
        print(f"Latest Date Found: {results['latest_date']}")
        print(f"Estimated Coverage: {results['total_years']:.1f} years")
        
        print(f"\nData Availability by Year:")
        for year_data in results['data_availability']:
            print(f"  {year_data['year']}: {year_data['count']} results")
            if year_data['sample']:
                print(f"    Sample: {year_data['sample'][:2]}")
        
        print(f"\nPagination Info:")
        if results['pagination_info']['has_pagination']:
            print(f"  Navigation Type: {results['pagination_info']['navigation_type']}")
            if results['pagination_info']['year_links']:
                print(f"  Year Links Found: {len(results['pagination_info']['year_links'])}")
        else:
            print("  No pagination detected")
    
    # Create daily update system
    print(f"\n{'='*60}")
    print("üîÑ Creating Daily Update System...")
    update_script = investigator.create_daily_update_system()
    print(f"‚úÖ Created: {update_script}")
    
    print(f"\nüíæ Database created: {investigator.db_path}")
    print("\nüéØ RECOMMENDATIONS:")
    print("1. LotteryUSA.com appears to have recent data (2021+)")
    print("2. For pre-2021 data, use your OCR system with PDF files")
    print("3. Set up daily updates using the created script")
    print("4. Combine OCR historical data (2000-2021) with daily updates (2021+)")

if __name__ == "__main__":
    main()